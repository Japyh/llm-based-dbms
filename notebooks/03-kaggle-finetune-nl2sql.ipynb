{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d077ae",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup & Repository Clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f6424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/Japyh/llm-based-dbms.git\n",
    "%cd llm-based-dbms\n",
    "\n",
    "# Verify data files exist\n",
    "!ls -lh\n",
    "!ls -lh data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install base dependencies from requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install fine-tuning specific packages\n",
    "!pip install -q transformers datasets accelerate bitsandbytes peft trl scikit-learn\n",
    "\n",
    "# Verify installations\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b09596",
   "metadata": {},
   "source": [
    "## Section 2: Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11fd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(\".\")\n",
    "from src.config import BASE_HF_NL2SQL_MODEL\n",
    "\n",
    "print(f\"Base model for fine-tuning: {BASE_HF_NL2SQL_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the chat-style NLâ†’SQL dataset\n",
    "dataset_path = Path(\"data/nl2sql_train_chat_raw.jsonl\")\n",
    "\n",
    "examples = []\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        examples.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(examples)} examples\")\n",
    "print(f\"\\nFirst example structure:\")\n",
    "print(json.dumps(examples[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b645324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process examples: separate prompt messages from response\n",
    "processed = []\n",
    "for ex in examples:\n",
    "    messages = ex[\"messages\"]\n",
    "    # System + User = prompt, Assistant = response\n",
    "    prompt_messages = messages[:-1]  # All except last (assistant)\n",
    "    response = messages[-1][\"content\"]  # Assistant's SQL\n",
    "    \n",
    "    processed.append({\n",
    "        \"prompt_messages\": prompt_messages,\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(processed)} examples\")\n",
    "print(f\"\\nSample processed example:\")\n",
    "print(f\"Prompt: {processed[0]['prompt_messages']}\")\n",
    "print(f\"Response: {processed[0]['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation (80/20)\n",
    "train_data, val_data = train_test_split(\n",
    "    processed,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train examples: {len(train_data)}\")\n",
    "print(f\"Validation examples: {len(val_data)}\")\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "print(f\"\\nDataset structure: {dataset_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09166cd8",
   "metadata": {},
   "source": [
    "## Section 3: Load Tokenizer and Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ea83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Optional: HF token for gated models (set in Kaggle Secrets as HF_TOKEN if needed)\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "\n",
    "print(f\"Loading tokenizer from {BASE_HF_NL2SQL_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_HF_NL2SQL_MODEL,\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "# Ensure pad_token is set (use eos_token as fallback)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Set pad_token to eos_token\")\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Loading base model {BASE_HF_NL2SQL_MODEL} with 4-bit quantization...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_HF_NL2SQL_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "print(f\"Model device map: {model.hf_device_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b1d19",
   "metadata": {},
   "source": [
    "## Section 4: Format Dataset with Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_example(example):\n",
    "    \"\"\"\n",
    "    Format a single example using the model's chat template.\n",
    "    Creates a 'text' field with prompt + response for training.\n",
    "    \"\"\"\n",
    "    prompt_messages = example[\"prompt_messages\"]\n",
    "    response = example[\"response\"]\n",
    "    \n",
    "    # Apply chat template to get formatted prompt\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        prompt_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Combine prompt + response for training\n",
    "    text = prompt + response\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting to both train and validation sets\n",
    "formatted_ds = dataset_dict.map(\n",
    "    format_chat_example,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(f\"Formatted dataset: {formatted_ds}\")\n",
    "print(f\"\\nSample formatted text (first 500 chars):\")\n",
    "print(formatted_ds[\"train\"][0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621ac4f",
   "metadata": {},
   "source": [
    "## Section 5: Configure LoRA and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Prepare model for k-bit training (required for QLoRA)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                      # LoRA rank\n",
    "    lora_alpha=16,             # LoRA alpha (scaling factor)\n",
    "    lora_dropout=0.1,          # Dropout for LoRA layers\n",
    "    bias=\"none\",               # Don't train biases\n",
    "    task_type=\"CAUSAL_LM\",     # Causal language modeling\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Apply LoRA to attention matrices\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c5adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./nl2sql-mistral-lora\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # Effective batch size: 2Ã—4 = 8\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    bf16=True,                      # Use bfloat16 mixed precision\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",          # Fixed: was evaluation_strategy (deprecated)\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",               # No external logging\n",
    "    warmup_steps=10,\n",
    "    optim=\"paged_adamw_8bit\",       # Memory-efficient optimizer\n",
    "    dataset_text_field=\"text\",      # Text field name in dataset\n",
    "    max_length=1024,                # Changed from max_seq_length (correct SFTConfig param)\n",
    ")\n",
    "\n",
    "print(\"Training Arguments:\")\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd06144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SFTTrainer (Supervised Fine-Tuning Trainer from TRL)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=formatted_ds[\"train\"],\n",
    "    eval_dataset=formatted_ds[\"validation\"],\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer initialized successfully!\")\n",
    "print(f\"Number of training examples: {len(formatted_ds['train'])}\")\n",
    "print(f\"Number of validation examples: {len(formatted_ds['validation'])}\")\n",
    "print(f\"\\nEstimated training steps: {len(formatted_ds['train']) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6343a",
   "metadata": {},
   "source": [
    "## Section 6: Run Training\n",
    "\n",
    "**Note**: This will take 30-60 minutes on Kaggle T4Ã—2 depending on your dataset size and hyperparameters.\n",
    "\n",
    "If you encounter **GPU OOM errors**, try:\n",
    "- Reduce `max_seq_length` to 512 or 768\n",
    "- Reduce `per_device_train_batch_size` to 1\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f0b71",
   "metadata": {},
   "source": [
    "## Section 7: Save Fine-Tuned Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned LoRA adapter and tokenizer\n",
    "output_dir = Path(\"nl2sql-mistral-lora\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving fine-tuned adapter to {output_dir}...\")\n",
    "\n",
    "# Save adapter weights\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "# Save tokenizer (important for inference)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"âœ“ Adapter and tokenizer saved!\")\n",
    "print(f\"\\nOutput directory contents:\")\n",
    "for item in sorted(output_dir.iterdir()):\n",
    "    size = item.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "    print(f\"  {item.name:40s} {size:8.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3884b62",
   "metadata": {},
   "source": [
    "## Section 8: Quick Smoke Test (Optional)\n",
    "\n",
    "Test the fine-tuned model with a sample question to verify it works before downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ea985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for inference\n",
    "print(\"Preparing model for inference...\")\n",
    "\n",
    "# The model from trainer is already a PEFT model, just put in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Fix dtype mismatch: Convert lm_head to bfloat16 to match compute dtype\n",
    "# For PEFT models, we need to access the base model properly\n",
    "try:\n",
    "    # PEFT models have a base_model attribute that wraps the actual model\n",
    "    if hasattr(model, 'base_model') and hasattr(model.base_model, 'model'):\n",
    "        # Structure: PeftModel.base_model (LoraModel).model (MistralForCausalLM)\n",
    "        base = model.base_model.model\n",
    "    elif hasattr(model, 'model'):\n",
    "        # Alternative: direct .model access\n",
    "        base = model.model\n",
    "    else:\n",
    "        # Fallback: model is already the base\n",
    "        base = model\n",
    "    \n",
    "    # Convert lm_head to bfloat16\n",
    "    if hasattr(base, 'lm_head'):\n",
    "        base.lm_head = base.lm_head.to(torch.bfloat16)\n",
    "        print(f\"âœ“ Converted lm_head to bfloat16 (dtype: {base.lm_head.weight.dtype})\")\n",
    "    else:\n",
    "        print(\"Warning: Could not find lm_head attribute\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not convert lm_head dtype: {e}\")\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ“ Model ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nl2sql(question: str):\n",
    "    \"\"\"\n",
    "    Test the fine-tuned model with a natural language question.\n",
    "    \"\"\"\n",
    "    # Build messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Text-to-SQL assistant for our SQLite sales database. \"\n",
    "                      \"Return only a valid SQL SELECT query, with no explanation, no comments, \"\n",
    "                      \"and no natural language. Never modify data or schema.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True)\n",
    "    \n",
    "    # Move inputs to model's device\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use the PEFT model's generate method\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (exclude prompt)\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"Testing fine-tuned model...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a77274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a few example questions\n",
    "test_questions = [\n",
    "    \"Show me the top 10 customers by total sales.\",\n",
    "    \"How many products are in each product line?\",\n",
    "    \"What is the total revenue?\",\n",
    "    \"List all customers from France.\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"Test {i}: {question}\")\n",
    "    sql = test_nl2sql(question)\n",
    "    print(f\"SQL: {sql}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2298e9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Download the adapter**: In Kaggle, go to Output â†’ Download `nl2sql-mistral-lora/`\n",
    "2. **Place in your repo**: Put the downloaded folder at `models/nl2sql-mistral-lora/`\n",
    "3. **Implement LocalHFLLMProvider**: Complete the TODO sections in `src/llm/provider.py`\n",
    "4. **Run evaluation**: Use `notebooks/04-kaggle-eval-nl2sql.ipynb` to validate performance\n",
    "5. **Integrate**: Wire up `NL2SQLEngine` to use `LocalHFLLMProvider`\n",
    "\n",
    "**Congratulations!** ðŸŽ‰ You've successfully fine-tuned a specialized NLâ†’SQL model!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
