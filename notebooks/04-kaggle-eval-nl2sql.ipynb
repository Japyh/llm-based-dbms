{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4c5a63",
   "metadata": {},
   "source": [
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c51e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17037b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Japyh/llm-based-dbms.git\n",
    "%cd llm-based-dbms\n",
    "!ls -lh data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1deefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "!pip install -q transformers datasets accelerate bitsandbytes peft trl scikit-learn\n",
    "\n",
    "import transformers\n",
    "import peft\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy or link the fine-tuned adapter\n",
    "# Option 1: If running right after training notebook, adapter is already in ./nl2sql-mistral-lora\n",
    "# Option 2: If uploaded as Kaggle dataset, copy it here\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "adapter_dir = Path(\"nl2sql-mistral-lora\")\n",
    "\n",
    "# Check if adapter exists\n",
    "if adapter_dir.exists():\n",
    "    print(f\"✓ Adapter found at {adapter_dir}\")\n",
    "    print(f\"Contents: {list(adapter_dir.iterdir())}\")\n",
    "else:\n",
    "    print(f\"⚠ Adapter not found at {adapter_dir}\")\n",
    "    print(\"If you uploaded it as a Kaggle dataset, copy it here:\")\n",
    "    print(\"  !cp -r /kaggle/input/your-adapter-dataset/nl2sql-mistral-lora .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07392130",
   "metadata": {},
   "source": [
    "## Section 2: Load Model and Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from src.config import BASE_HF_NL2SQL_MODEL\n",
    "\n",
    "print(f\"Base model: {BASE_HF_NL2SQL_MODEL}\")\n",
    "print(f\"Adapter directory: {adapter_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57aea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_dir)\n",
    "print(f\"✓ Tokenizer loaded from adapter directory\")\n",
    "\n",
    "# For evaluation, we can use 4-bit to save memory (or 16-bit for faster inference)\n",
    "# Using 4-bit here for consistency with training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(f\"Loading base model {BASE_HF_NL2SQL_MODEL}...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_HF_NL2SQL_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Loading LoRA adapter from {adapter_dir}...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)\n",
    "model.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befef709",
   "metadata": {},
   "source": [
    "## Section 3: Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the full dataset\n",
    "dataset_path = Path(\"data/nl2sql_train_chat_raw.jsonl\")\n",
    "examples = []\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        examples.append(json.loads(line))\n",
    "\n",
    "print(f\"Total examples: {len(examples)}\")\n",
    "\n",
    "# Use same split as training (20% validation)\n",
    "_, test_examples = train_test_split(\n",
    "    examples,\n",
    "    test_size=0.2,\n",
    "    random_state=42  # Same seed as training\n",
    ")\n",
    "\n",
    "print(f\"Test examples: {len(test_examples)}\")\n",
    "print(f\"\\nFirst test example:\")\n",
    "print(json.dumps(test_examples[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a6b3d",
   "metadata": {},
   "source": [
    "## Section 4: Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb3514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sql(question: str, system_prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate SQL from a natural language question.\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"You are a Text-to-SQL assistant for our SQLite sales database. \"\n",
    "            \"Return only a valid SQL SELECT query, with no explanation, no comments, \"\n",
    "            \"and no natural language. Never modify data or schema.\"\n",
    "        )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"✓ Generation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f031a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.db.core import run_query\n",
    "\n",
    "def is_sql_executable(sql: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if SQL query executes without error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        run_query(sql)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def normalize_sql(sql: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize SQL for comparison (lowercase, strip whitespace).\n",
    "    \"\"\"\n",
    "    return \" \".join(sql.lower().split())\n",
    "\n",
    "print(\"✓ Evaluation utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003d863",
   "metadata": {},
   "source": [
    "## Section 5: Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91aeefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on all test examples\n",
    "results = []\n",
    "\n",
    "print(f\"Evaluating on {len(test_examples)} test examples...\\n\")\n",
    "\n",
    "for i, example in enumerate(test_examples):\n",
    "    messages = example[\"messages\"]\n",
    "    system_msg = messages[0][\"content\"]\n",
    "    user_msg = messages[1][\"content\"]\n",
    "    reference_sql = messages[2][\"content\"]\n",
    "    \n",
    "    # Generate SQL\n",
    "    generated_sql = generate_sql(user_msg, system_msg)\n",
    "    \n",
    "    # Evaluate\n",
    "    is_executable = is_sql_executable(generated_sql)\n",
    "    is_exact_match = normalize_sql(generated_sql) == normalize_sql(reference_sql)\n",
    "    \n",
    "    result = {\n",
    "        \"question\": user_msg,\n",
    "        \"reference_sql\": reference_sql,\n",
    "        \"generated_sql\": generated_sql,\n",
    "        \"is_executable\": is_executable,\n",
    "        \"is_exact_match\": is_exact_match\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(test_examples)} examples...\")\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1172f",
   "metadata": {},
   "source": [
    "## Section 6: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "total = len(results)\n",
    "executable_count = sum(1 for r in results if r[\"is_executable\"])\n",
    "exact_match_count = sum(1 for r in results if r[\"is_exact_match\"])\n",
    "\n",
    "executable_rate = (executable_count / total) * 100\n",
    "exact_match_rate = (exact_match_count / total) * 100\n",
    "\n",
    "metrics = {\n",
    "    \"total_examples\": total,\n",
    "    \"executable_count\": executable_count,\n",
    "    \"executable_rate\": executable_rate,\n",
    "    \"exact_match_count\": exact_match_count,\n",
    "    \"exact_match_rate\": exact_match_rate\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total test examples:        {total}\")\n",
    "print(f\"Executable SQL queries:     {executable_count} ({executable_rate:.1f}%)\")\n",
    "print(f\"Exact match SQL:            {exact_match_count} ({exact_match_rate:.1f}%)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ad4dd",
   "metadata": {},
   "source": [
    "## Section 7: Show Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f124b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show a few random examples\n",
    "sample_results = random.sample(results, min(5, len(results)))\n",
    "\n",
    "for i, result in enumerate(sample_results, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Reference SQL: {result['reference_sql']}\")\n",
    "    print(f\"Generated SQL: {result['generated_sql']}\")\n",
    "    print(f\"Executable: {result['is_executable']} | Exact Match: {result['is_exact_match']}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1093dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show failures (if any)\n",
    "failures = [r for r in results if not r[\"is_executable\"]]\n",
    "\n",
    "if failures:\n",
    "    print(f\"\\n⚠ {len(failures)} FAILED QUERIES (non-executable):\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, result in enumerate(failures[:5], 1):  # Show first 5\n",
    "        print(f\"\\nFailure {i}:\")\n",
    "        print(f\"Question: {result['question']}\")\n",
    "        print(f\"Generated SQL: {result['generated_sql']}\")\n",
    "        print(\"-\" * 70)\n",
    "else:\n",
    "    print(\"\\n✓ All generated queries are executable!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749f17d",
   "metadata": {},
   "source": [
    "## Section 8: Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics to JSON\n",
    "output_file = Path(\"nl2sql_eval_metrics.json\")\n",
    "\n",
    "eval_output = {\n",
    "    \"metrics\": metrics,\n",
    "    \"sample_results\": results[:10],  # Save first 10 for inspection\n",
    "    \"failures\": failures[:10] if failures else []\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eval_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✓ Evaluation results saved to {output_file}\")\n",
    "print(f\"\\nYou can download this file from Kaggle's Output section.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5588bbc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Download evaluation metrics**: Get `nl2sql_eval_metrics.json` from Output\n",
    "2. **Document results**: Add metrics to your README or project documentation\n",
    "3. **If results are good**:\n",
    "   - Deploy the adapter to `models/nl2sql-mistral-lora/` in your repo\n",
    "   - Implement full `LocalHFLLMProvider` in `src/llm/provider.py`\n",
    "   - Integrate with `NL2SQLEngine`\n",
    "4. **If results need improvement**:\n",
    "   - Increase training epochs\n",
    "   - Expand dataset with more examples\n",
    "   - Try different LoRA hyperparameters\n",
    "   - Experiment with different base models\n",
    "\n",
    "### Typical Results to Expect\n",
    "\n",
    "For a well-fine-tuned model on 220 examples:\n",
    "- **Executable rate**: 85-95% (most queries run without syntax errors)\n",
    "- **Exact match rate**: 30-60% (exact string match is strict; semantic equivalence is higher)\n",
    "\n",
    "Even if exact match is lower, queries that are executable and produce correct results are still valuable!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
